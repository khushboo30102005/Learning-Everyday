# ReLU (Rectified Linear Unit) Activation Function

## ðŸ“Œ Introduction
The **ReLU (Rectified Linear Unit)** activation function is one of the most commonly used activation functions in **Deep Learning and Neural Networks**.  
It introduces non-linearity into the model, allowing neural networks to learn complex patterns efficiently.

---

## ðŸ“ Mathematical Definition
The ReLU function is defined as:

\[
ReLU(x) = \max(0, x)
\]

### Explanation:
- If `x > 0`, output = `x`
- If `x â‰¤ 0`, output = `0`

---

## ðŸ“Š Graph Interpretation
- For negative input values â†’ output is **0**
- For positive input values â†’ output is a **linear identity function**

This simple behavior makes ReLU computationally efficient.

---

## âœ… Why ReLU is Popular?

### 1. Introduces Non-Linearity
Without activation functions like ReLU, neural networks would behave like linear models.

### 2. Solves Vanishing Gradient Problem
Unlike sigmoid or tanh, ReLU allows gradients to flow better for positive values.

### 3. Faster Computation
ReLU uses simple mathematical operations, making training faster.

### 4. Sparse Activation
Many neurons output zero, making the network more efficient.

---

## âš ï¸ Limitations of ReLU

### âŒ Dying ReLU Problem
- When neurons receive negative inputs consistently, they output zero forever.
- Such neurons stop learning.

---

## ðŸ” Variants of ReLU

| Variant | Description |
|------|------------|
| **Leaky ReLU** | Allows small negative slope |
| **Parametric ReLU (PReLU)** | Learnable negative slope |
| **ELU** | Smooth curve for negative inputs |

---

## ðŸ“Œ Use Cases
- Hidden layers of Deep Neural Networks
- Convolutional Neural Networks (CNNs)
- Computer Vision tasks
- Speech and Image Recognition

---

## ðŸ§  Summary
- ReLU is simple, fast, and effective
- Widely used in modern deep learning models
- Has limitations, but variants address them

---

## ðŸ“š Key Takeaway
> *ReLU made deep learning practical by enabling faster and more stable training.*



